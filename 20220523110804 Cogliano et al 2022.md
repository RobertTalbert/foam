# Cogliano et al. 2022 

>Cogliano, M., Bernacki, M. L., Hilpert, J. C., & Strong, C. L. (2022). A self-regulated learning analytics prediction-and-intervention design: Detecting and supporting struggling biology students._Journal of Educational Psychology_.

#learning-analytics #self-regulation #predictive-model #biology

## Notes 

+ Intro/background
    + Basic question: Can you make an intervention for students who might need it, that's BRIEF, and likely to work based on predictive modeling where only a handful of learning events are needed? 
    + SRL involves three basic cyclical phases: 
        + Preparatory
        + Performance 
        + Appraisal
    + There are interventions in this area already but they either only have a small effect, or they are really long and involved. (And typically don't involve LA data or any kind of data.)
    + Questions: 
        + Do students who complete learning skills training outperform those predicted to perform poorly but who didn't do the training? (I.e. did the training work?) 
        + How do students who were *not* predicted to perform poorly, perform compared to those who were flagged (regardless of whether they received training)? (I.e. is the prediction valid?)
        + Among those flagged to perform poorly, do those who complete training do better gradewise than those who didn't complete the training? (i.e. did the intervention "work")
        + How does prediction and training influence proportion of students earning desirable and undesirable grades? (i.e. does the prediction and training "move the needle"?)
+ Method
    + 226 undergraduates in biology intro course 
    + 63% flagged as more likely to earn a C or worse based on...?
    + Of those flagged, ⅔ assigned to treatment condition
    + 79 flagged treatment students, 64 flagged control, 83 nonflagged 
    + Measures
        + Pretest – bunch of different problems including MC and algebra questions – no SS difference in pretesting between flagged treatment/flagged control
        + Exams – four unit tests and a final 
        + Learning events: Access of resources (and that's all?) 
        + Check-in activity 
            + Self report of study methods, feeling of preparation, grade expectations – this was done early
            + The treatment gropu had an additional module constituting the intervention
    + Prediction model was built before the intervention study using pretest and behavioral data in first 2 weeks of the course
      + Model was applied to students who consented to the study
      + They were flagged/not flagged on the basis of the model output 
      + Model built around learning events, interactions with: announcements, files, discussion posts, tools, etc. 
      + The best model contained nine predictors and accurately classified 74% of students based on achievement of course grade of B or better/C or worse 
        + Module view (week before course)
        + Prior knowledge based on pretest score
        + Announcement view in week 1
        + Assignment view in week 1
        + External tool view in week 1
        + Files view in week 1
        + Grades view in week 1
        + Total number of events in LMS overall in week 1
        + Discussion topic view in week 2 
    + Intervention for the flagged treatment group was a brief tutorial on "Science of Learning to Learn" (average completion time 15.8 minutes), followed up with response to prompt about what they will do next to prepare for the Exams
  + RESULTS
    + Flagged treatment performed SS better than Flagged Control on unit tests and final
    + SS differences between groups and final grade
    + Flagged control had SS lower course grades than flagged treatment
    + Flagged treatment and non-flagged were not SS different in course grade 
  + LIMITATIONS
    + Single course in STEM with active learning
    + Single large lecture course at a large university