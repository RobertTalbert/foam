# Van Amburgh et al. 2007

>Van Amburgh, J. A., Devlin, J. W., Kirwin, J. L., & Qualters, D. M. (2007). A tool for measuring active learning in the classroom._American journal of pharmaceutical education_,_71_(5).

#active-learning #teaching #instruments
#quantitative #research

## Skim-through notes 

- Done for pharmacy – does that affect validity? 
- What is a "Didaktik triangle"
- In the discussion, there is a mention that they foresee the ALIT being used outside professional programs (health, science, and humanities)
- Also it was designed for large classes (100+) – does it work as well with smaller ones

* Definitely follow up by reading Weiman and Gilbert 2017 https://www.lifescied.org/doi/full/10.1187/cbe.14-02-0023 

## Notes 

+ What is it
    - [Didaktik triangle](https://www.researchgate.net/figure/Didaktik-triangle-Kuenzli-1998_fig1_335729020)
    - Even in 2007 there was a lot of belief and evidence of the effectiveness of active learning
    - But no inventory to allow qualitative evaluation.
    - Such a tool if it existed would
        - Make qualitative evaluation possible
        - Help determine the type of AL that is best suited to teach or assess a particular level of knowledge
        - Aid in the evaluation of faculty and programs
        - Be a research tool
        - Provide documentation for increasing emphasis on measuring outcomes ("in pharmacy education" but really anywhere)
    - Purpose: " to develop a valid and reliable active-learning inventory tool to quantify the use of active learning in large courses" (note the last part)
    - Secondary use: Compare instructor perceptions of their use of AL with actual measurement
    - "CER" components: (1) The Context of the activity is explained, (2) students were Engaged, (3) there was closure via Reflection -- fits with Bonwell/Eison definition 
    
+ What did they do
    - Rooted in "change theory"
    - "Each active-learning technique was initially categorized according to faculty risk, which was estimated based on the intensity and ease of use of the activity in the classroom." -- Later this was changed to "complexity" but the term "risk" is apt
    - Lectures were selected during the summer 2005, fall 2005, or spring 2006 semesters when at least three fourths of the observers could attend and when the instructor was willing to participate. Nine lectures (3 videotaped and 6 live) were used for reliability evaluation. 
    - Students were in the third-professional year (P3) of a doctor of pharmacy degree program with an approximate class size of 100.
    - Seven of the instructors were interviewed following their lecture using a scripted interview guide to elicit perceptions of their lesson that included: their definition of active learning, the perceived merits of active learning in the classroom, the types of active-learning activities used in the lecture, the rationale for the use of the specific active-learning activities chosen, the estimated amount of class time that was devoted to active-learning activities, the estimated time required to prepare the lesson and active-learning activities, any perceived barriers to the use of active learning, and the impact of using active-learning techniques on the amount of content covered.

+ What happened/what was learned
    - See Table 1 for modifications made after pilot
    - Over these 9 lectures, an average of 13 (range: 4-34) episodes of active learning were observed that took an average of 2.2 minutes (range: 0.6-16) each to complete. Three (range: 2-5) different types of active learning were observed per lecture.

+ Questions
    - Has this been validated on smaller courses? Courses outside STEM? 
    - How were the evaluators trained? 
    - Big question for me: Does it scale up? 
        - It's expensive to do this right. You have to train observersYou have to identify a large enough sample size to be able to make conclusions about the population You have to understand what the population is. You could give this out as a self-reporting survey but then you would lose much of the observational validity that you would have by having trained observers. 
        - So let's suppose you wanted to use the survey instrument to answer the question, how much active learning is taking place at my university? You would have to first identify what is the population. That seems like it would be just the sheer number of class sessions that are offered on a semesterly basis. So do the math on that and a three credit class. There are three sessions per week times 15 weeks which is 45 sessions per three credit class. Just for the math department, multiply that by 30 faculty and then by 3 classes per faculty member. Call it 40 sessions to account for tests etc. So the number of class sessions is 40 times 30 times 3, which is 3600. [To draw a sample from that pool](https://www.surveysystem.com/sscalc.htm) large enough to make an inference with 95% confidence and a MOE of 10%, you would need 94 observations. That's for one department. To do that, it averages to about 7 observations per week. So if you enlisted 3 people as observers and have them observe 2 classes per week, randomly selected for 15 weeks, you'd have enough of a sample for a reasonable guess for one department. 
        - Which is doable! But what do you get? Information with a fairly high margin of error for one department. 
        - So let's suppose you want to do math, physics, chem, bio, and BMS. Don't use labs or seminar courses. Assume that everybody else teaches roughly the same number of classes as we do, so the total population size is 3600*5 = 18000. **I AM SHOCKED** to see that the sample size is the same! I did not expect that. This can't be right, can it? 
        - Will ask questions of stats people to help me understand what's happening there. But it seems doable to use this instrument. 


+ References 
    - Prochaska JO, DiClemente CC, Norcross JC. In search of how people change. Am Psychol. 1992;47:1102–14.
    - Qualters DM, Sheahan T, Isaacs J. An electronic advice column to effect teaching culture change. To Improve the Academy (Professional and Organizational Development Network, http://www.podnetwork.org). 2006;24:201-16.